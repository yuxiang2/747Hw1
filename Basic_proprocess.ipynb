{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file_name = 'origin_data/origin_train.txt'\n",
    "valid_file_name = 'origin_data/origin_valid.txt'\n",
    "text_file_name = 'origin_data/origin_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_labels = []\n",
    "valid_texts = []\n",
    "\n",
    "with codecs.open(valid_file_name, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        splits = str.split(line, ' ||| ')\n",
    "        valid_labels.append(splits[0])\n",
    "        valid_texts.append(splits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "train_texts = []\n",
    "\n",
    "with codecs.open(train_file_name, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        splits = str.split(line, ' ||| ')\n",
    "        train_labels.append(splits[0])\n",
    "        train_texts.append(splits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_texts = []\n",
    "\n",
    "with codecs.open(text_file_name, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        splits = str.split(line, ' ||| ')\n",
    "        test_texts.append(splits[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuations Stop Words, Add POS tags, Lower cases, Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tag.mapping import tagset_mapping\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pos_map = tagset_mapping('en-ptb', 'universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize(word, tag, lemmatizer=lemmatizer):\n",
    "    word = word.lower()\n",
    "    if tag == 'NOUN':\n",
    "        word = lemmatizer.lemmatize(word, pos='n')\n",
    "    elif tag == 'VERB':\n",
    "        word = lemmatizer.lemmatize(word, pos='v')\n",
    "    elif tag == 'ADJ':\n",
    "        word = lemmatizer.lemmatize(word, pos='a')\n",
    "    elif tag == 'ADV':\n",
    "        word = lemmatizer.lemmatize(word, pos='r')\n",
    "    elif tag == 'NUM':\n",
    "        try:\n",
    "            word = float(word)\n",
    "            word = str(int(math.log10(abs(word)+1)))\n",
    "        except:\n",
    "            pass\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(sent, tokenizer=tokenizer):\n",
    "    words = []\n",
    "    for word in str.split(sent, ' '):\n",
    "        if word == '@.@':\n",
    "            words.append('.')\n",
    "            continue\n",
    "        if word == '@,@':\n",
    "            words.append(',')\n",
    "            continue\n",
    "        if word != '@-@':\n",
    "            words.append(word)\n",
    "    sent = ' '.join(words)\n",
    "    words = tokenizer(sent)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    new_words = []\n",
    "    for pos_tag in pos_tags:\n",
    "        word, tag = pos_tag\n",
    "        tag = pos_map[tag]\n",
    "        word = lemmatize(word, tag)\n",
    "            \n",
    "#         if word not in stop_words:\n",
    "        new_words.append('_'.join([word, tag]))\n",
    "\n",
    "    return ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open('valid.txt', 'w', encoding='utf8') as f:\n",
    "    for line in valid_texts:\n",
    "        f.write(preprocess(line)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253909/253909\r"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "with codecs.open('train_no_oov.txt', 'w', encoding='utf8') as f:\n",
    "    for line in train_texts:\n",
    "        f.write(preprocess(line)+'\\n')\n",
    "        count += 1\n",
    "        print('{}/{}'.format(count, len(train_texts)),end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open('test.txt', 'w', encoding='utf8') as f:\n",
    "    for line in test_texts:\n",
    "        f.write(preprocess(line)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Set of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = set(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Agriculture, food and drink', 'Music', 'Language and literature', 'Geography and places', 'History', 'Art and architecture', 'Video games', 'Mathematics', 'Social sciences and society', 'Natural sciences', 'Philosophy and religion', 'Warfare', 'Media and drama', 'Engineering and technology', 'Sports and recreation', 'Miscellaneous']\n"
     ]
    }
   ],
   "source": [
    "num2labels = list(labels)\n",
    "print(num2labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels2num = {}\n",
    "for i,label in enumerate(num2labels):\n",
    "    labels2num[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Agriculture, food and drink': 0,\n",
       " 'Art and architecture': 5,\n",
       " 'Engineering and technology': 13,\n",
       " 'Geography and places': 3,\n",
       " 'History': 4,\n",
       " 'Language and literature': 2,\n",
       " 'Mathematics': 7,\n",
       " 'Media and drama': 12,\n",
       " 'Miscellaneous': 15,\n",
       " 'Music': 1,\n",
       " 'Natural sciences': 9,\n",
       " 'Philosophy and religion': 10,\n",
       " 'Social sciences and society': 8,\n",
       " 'Sports and recreation': 14,\n",
       " 'Video games': 6,\n",
       " 'Warfare': 11}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels2num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('valid_label.txt', 'w') as f:\n",
    "    for label in valid_labels:\n",
    "        if label == 'Media and darama':\n",
    "            label = 'Media and drama'\n",
    "        f.write(str(labels2num[label])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train_label.txt', 'w') as f:\n",
    "    for label in train_labels:\n",
    "        f.write(str(labels2num[label])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Infrequent Words to OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file_name = 'train_no_oov.txt'\n",
    "valid_file_name = 'valid.txt'\n",
    "text_file_name = 'test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "with codecs.open(train_file_name, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        words = str.split(line, ' ')\n",
    "        for word in words:\n",
    "            word = str.split(word, '_')[0]\n",
    "            vocab[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107139"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sasuke', 20),\n",
       " ('peacekeeper', 20),\n",
       " (\"'neill\", 20),\n",
       " ('45th', 20),\n",
       " ('martian', 20),\n",
       " ('scratch', 20),\n",
       " ('dissection', 20),\n",
       " ('missing', 20),\n",
       " ('che', 20),\n",
       " ('cola', 20)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = -95015\n",
    "vocab.most_common()[idx:idx+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### get frequent words\n",
    "good_vocab = vocab.most_common()[:idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good_vocab = [word for word,_ in good_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253909\r"
     ]
    }
   ],
   "source": [
    "f2 = open('train.txt', 'w', encoding='utf8')\n",
    "count = 0\n",
    "\n",
    "with codecs.open(train_file_name, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        if len(line.strip()) == 0:\n",
    "            f2.write('<None>' + '\\n')\n",
    "            count += 1\n",
    "            print('{}'.format(count), end='\\r')\n",
    "        \n",
    "        else:\n",
    "            words = str.split(line, ' ')\n",
    "            words[-1] = words[-1].strip()\n",
    "            new_line = []\n",
    "            for word in words:\n",
    "                try:\n",
    "                    word,tag = str.split(word, '_')\n",
    "                except:\n",
    "                    continue\n",
    "                if word in good_vocab:\n",
    "                    new_line.append(word)\n",
    "                else:\n",
    "                    new_line.append(tag)\n",
    "            f2.write(' '.join(new_line) + '\\n')\n",
    "\n",
    "            count += 1\n",
    "            print('{}'.format(count), end='\\r')\n",
    "            \n",
    "#             if count == 100:\n",
    "#                 break\n",
    "        \n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
